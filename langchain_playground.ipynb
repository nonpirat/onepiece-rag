{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for going through langchain tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Loading Necessary Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Kuma's female friend in One Piece is Nico Robin.\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who is Kuma's female friend in One Piece?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping from One Piece Fandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#Function for scraping one page, take in the url and also the interesting section\n",
    "response = requests.get('https://onepiece.fandom.com/wiki/Reverse_Mountain_Arc')\n",
    "\n",
    "page_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_element(element, prefix = ''):\n",
    "    \"\"\"Recursively extracts text from paragraphs and unordered lists.\"\"\"\n",
    "    text_content = ''\n",
    "    if element.name == 'p':\n",
    "        text_content += prefix + element.get_text().strip() + '\\n\\n'\n",
    "    elif element.name == 'ul':\n",
    "        for item in element.find_all('li', recursive=False):  # Direct children only to handle nesting\n",
    "            \n",
    "            item_text = ''.join(str(child) for child in item.children if child.name != 'ul')\n",
    "            item_soup = BeautifulSoup(item_text, 'html.parser')\n",
    "            text_content += prefix + '- ' + item_soup.get_text().strip() + '\\n'\n",
    "            # Check for nested <ul> within this <li>\n",
    "            nested_ul = item.find('ul')\n",
    "            if nested_ul:\n",
    "                text_content += extract_text_from_element(nested_ul, prefix = '\\t'+prefix)\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def scrape_page(url, topic = ['Summary', 'Story Impact']):\n",
    "    \"\"\"Scrapes paragraphs and nested unordered lists under a specific header and saves to a text file.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        page_title = soup.find('h1', {'class': \"page-header__title\"})\n",
    "        if page_title:\n",
    "            page_title = page_title.get_text().strip()\n",
    "            \n",
    "        headers = soup.find_all('h2')  # Adjust this to target the specific header you're interested in\n",
    "        \n",
    "        if headers:\n",
    "            titles = [header.get_text().strip().replace('[', '').replace(']', '') for header in headers]\n",
    "            content = '' #f'{page_title}\\n\\n\\n\\n'\n",
    "            if ('Long Summary' in topic) and ('Long Summary' not in titles):\n",
    "                if 'Summary' in titles:\n",
    "                    topic = ['Summary'] + topic\n",
    "                elif 'Short Summary' in titles:\n",
    "                    topic = ['Short Summary'] + topic\n",
    "                    \n",
    "            for j, header in enumerate(headers):\n",
    "                title = header.get_text().strip().replace('[', '').replace(']', '')\n",
    "                # print(title)\n",
    "                next_header = headers[j+1] if j+1 < len(headers) else None\n",
    "                if title in topic:\n",
    "                    content += title+'\\n\\n\\n'\n",
    "                    # Iterate over siblings of the header that are either <p> or <ul>, including nested <ul>\n",
    "                    for sibling in header.find_next_siblings():\n",
    "                        if sibling == next_header:\n",
    "                            break\n",
    "                        content += extract_text_from_element(sibling)\n",
    "\n",
    "                \n",
    "            # Save the content to a file\n",
    "            # with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            #     file.write(content)\n",
    "            content = re.sub(r'\\[.*?\\]', '', content) \n",
    "            return page_title, content\n",
    "        else:\n",
    "            return 'Header not found'\n",
    "    else:\n",
    "        return 'Failed to retrieve the linked page'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long Summary\n",
      "\n",
      "\n",
      "On an island somewhere in the Grand Line, Ace runs away from a restaurant after eating without paying, with the chef he ran from giving chase. When he reaches the dock, he stops to ask people for information on Blackbeard. However, his searching only leads him to find a man known as Dr. Black Beard. Not realizing that he has the wrong person, Ace kicks Dr. Black Beard, which causes the angry villagers to literally kick him into the river. Being a Devil Fruit user, Ace nearly drowns, but after going downstream, he is rescued by a young girl named Moda. She then takes the unconscious pirate to her house on her family's dairy farm, where he eventually wakes up. Ace and Moda officially meet, and Moda asks him to deliver a letter for her. In exchange for saving his life, Ace agrees to deliver a letter.\n",
      "\n",
      "Ace goes to the G-2 Marine base, where the letter is supposed to be delivered. He swiftly defeats a Marine soldier and takes his uniform as a disguise to infiltrate the base. He then finds himself in the dining hall, where he helps himself to some food. However, he overhears a Marine badmouthing Whitebeard and clobbers him, thus blowing his own cover. After managing to elude the Marines chasing him, he then beats up a Marine commander and taking his clothes as well while also eating his meal. In a different part of the Marine base, a meeting quickly turns sour because the coffee is extremely bitter. Even the commanding officer, Vice Admiral Comil, hates it.\n",
      "\n",
      "Elsewhere, a marine scout ship approaches the base, but it gets set on fire by some pirates. The Marines are very worried because top-secret documents are on the scout ship, when one of them jumps into the conflagration. The Marine turns out to be Ace, who was immune to the flame thanks to his abilities. He carries some documents as well as an unconscious Marine out while still on fire. However, this causes the Marines to quickly deduce Ace's identity and chase him down again, but he was able to evade their attacks thanks to his Logia powers. Before escaping, Ace hands Moda's letter to Comil, the intended recipient, while still being shot at. Comil reads Moda's letter, which asks the Marines if they would like to buy her milk.\n",
      "\n",
      "A Marine food ship stops by Moda's home to purchase her milk, and it is revealed that the cooks on the ship are Moda's parents, who happily embrace their daughter. Due to now having her milk, the Marines' coffee tastes much better. Meanwhile, Ace sails off, having gotten information on Blackbeard from the Marine scout ship, and is now ready to find and take on the traitorous pirate.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scrape_page('https://onepiece.fandom.com/wiki/Ace%27s_Great_Blackbeard_Search', topic=['Long Summary'])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://onepiece.fandom.com/wiki/Chapters_and_Volumes'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romance Dawn Arc(Chapters 1 to 7, Volume 1)\n",
      "Link: https://onepiece.fandom.com/wiki/Romance_Dawn_Arc\n",
      "Reverse Mountain Arc(Chapters 101 to 105, Volume 12)\n",
      "Link: https://onepiece.fandom.com/wiki/Reverse_Mountain_Arc\n",
      "Jaya Arc(Chapters 218 to 236, Volumes 24 and 25)\n",
      "Link: https://onepiece.fandom.com/wiki/Jaya_Arc\n",
      "Long Ring Long Land Arc(Chapters 303 to 321, Volumes 32 to 34)\n",
      "Link: https://onepiece.fandom.com/wiki/Long_Ring_Long_Land_Arc\n",
      "Thriller Bark Arc(Chapters 442 to 489, Volumes 46 to 50)\n",
      "Link: https://onepiece.fandom.com/wiki/Thriller_Bark_Arc\n",
      "Sabaody Archipelago Arc(Chapters 490 to 513, Volumes 50 to 53)\n",
      "Link: https://onepiece.fandom.com/wiki/Sabaody_Archipelago_Arc\n",
      "--- End of Row ---\n",
      "Orange Town Arc(Chapters 8 to 21, Volumes 1 to 3)\n",
      "Link: https://onepiece.fandom.com/wiki/Orange_Town_Arc\n",
      "Whisky Peak Arc(Chapters 106 to 114, Volumes 12 and 13)\n",
      "Link: https://onepiece.fandom.com/wiki/Whisky_Peak_Arc\n",
      "Skypiea Arc(Chapters 237 to 302, Volumes 26 to 32)\n",
      "Link: https://onepiece.fandom.com/wiki/Skypiea_Arc\n",
      "Water 7 Arc(Chapters 322 to 374, Volumes 34 to 39)\n",
      "Link: https://onepiece.fandom.com/wiki/Water_7_Arc\n",
      "\n",
      "Amazon Lily Arc(Chapters 514 to 524, Volumes 53 and 54)\n",
      "Link: https://onepiece.fandom.com/wiki/Amazon_Lily_Arc\n",
      "--- End of Row ---\n",
      "Syrup Village Arc(Chapters 22 to 41, Volumes 3 to 5)\n",
      "Link: https://onepiece.fandom.com/wiki/Syrup_Village_Arc\n",
      "Little Garden Arc(Chapters 115 to 129, Volumes 13 to 15)\n",
      "Link: https://onepiece.fandom.com/wiki/Little_Garden_Arc\n",
      "\n",
      "Enies Lobby Arc(Chapters 375 to 430, Volumes 39 to 44)\n",
      "Link: https://onepiece.fandom.com/wiki/Enies_Lobby_Arc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/712449126.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  item_soup = BeautifulSoup(item_text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Impel Down Arc(Chapters 525 to 549, Volumes 54 to 56)\n",
      "Link: https://onepiece.fandom.com/wiki/Impel_Down_Arc\n",
      "--- End of Row ---\n",
      "Baratie Arc(Chapters 42 to 68, Volumes 5 to 8)\n",
      "Link: https://onepiece.fandom.com/wiki/Baratie_Arc\n",
      "Drum Island Arc(Chapters 130 to 154, Volumes 15 to 17)\n",
      "Link: https://onepiece.fandom.com/wiki/Drum_Island_Arc\n",
      "\n",
      "Post-Enies Lobby Arc(Chapters 431 to 441, Volumes 45 and 46)\n",
      "Link: https://onepiece.fandom.com/wiki/Post-Enies_Lobby_Arc\n",
      "\n",
      "Marineford Arc(Chapters 550 to 580, Volumes 56 to 59)\n",
      "Link: https://onepiece.fandom.com/wiki/Marineford_Arc\n",
      "--- End of Row ---\n",
      "Arlong Park Arc(Chapters 69 to 95, Volumes 8 to 11)\n",
      "Link: https://onepiece.fandom.com/wiki/Arlong_Park_Arc\n",
      "Arabasta Arc(Chapters 155 to 217, Volumes 17 to 24)\n",
      "Link: https://onepiece.fandom.com/wiki/Arabasta_Arc\n",
      "\n",
      "\n",
      "\n",
      "Post-War Arc(Chapters 581 to 597, Volumes 59 to 61)\n",
      "Link: https://onepiece.fandom.com/wiki/Post-War_Arc\n",
      "--- End of Row ---\n",
      "Loguetown Arc(Chapters 96 to 100, Volumes 11 and 12)\n",
      "Link: https://onepiece.fandom.com/wiki/Loguetown_Arc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- End of Row ---\n",
      "Return to Sabaody Arc(Chapters 598 to 602, Volume 61)\n",
      "Link: https://onepiece.fandom.com/wiki/Return_to_Sabaody_Arc\n",
      "Punk Hazard Arc(Chapters 654 to 699, Volumes 66 to 70)\n",
      "Link: https://onepiece.fandom.com/wiki/Punk_Hazard_Arc\n",
      "Zou Arc(Chapters 802 to 824, Volumes 80 to 82)\n",
      "Link: https://onepiece.fandom.com/wiki/Zou_Arc\n",
      "Wano Country Arc(Chapters 909 to 1057, Volumes 90 to 105)\n",
      "Link: https://onepiece.fandom.com/wiki/Wano_Country_Arc\n",
      "Egghead Arc(Chapters 1058 to Current, Volume 105 to Current)\n",
      "Link: https://onepiece.fandom.com/wiki/Egghead_Arc\n",
      "--- End of Row ---\n",
      "Fish-Man Island Arc(Chapters 603 to 653, Volumes 61 to 66)\n",
      "Link: https://onepiece.fandom.com/wiki/Fish-Man_Island_Arc\n",
      "Dressrosa Arc(Chapters 700 to 801, Volumes 70 to 80)\n",
      "Link: https://onepiece.fandom.com/wiki/Dressrosa_Arc\n",
      "Whole Cake Island Arc(Chapters 825 to 902, Volumes 82 to 90)\n",
      "Link: https://onepiece.fandom.com/wiki/Whole_Cake_Island_Arc\n",
      "\n",
      "\n",
      "--- End of Row ---\n",
      "\n",
      "\n",
      "Levely Arc(Chapters 903 to 908, Volume 90)\n",
      "Link: https://onepiece.fandom.com/wiki/Levely_Arc\n",
      "\n",
      "\n",
      "--- End of Row ---\n",
      "Link: https://onepiece.fandom.com/wiki/Buggy%27s_Crew_Adventure_Chronicles\n",
      "Link: https://onepiece.fandom.com/wiki/Diary_of_Koby-Meppo\n",
      "Link: https://onepiece.fandom.com/wiki/Jango%27s_Dance_Paradise\n",
      "Link: https://onepiece.fandom.com/wiki/Hatchan%27s_Sea-Floor_Stroll\n",
      "Link: https://onepiece.fandom.com/wiki/Wapol%27s_Omnivorous_Hurrah\n",
      "Link: https://onepiece.fandom.com/wiki/Ace%27s_Great_Blackbeard_Search\n",
      "Link: https://onepiece.fandom.com/wiki/Gedatsu%27s_Accidental_Blue-Sea_Life\n",
      "Link: https://onepiece.fandom.com/wiki/Miss_Goldenweek%27s_%22Operation:_Meet_Baroque_Works%22\n",
      "Link: https://onepiece.fandom.com/wiki/Enel%27s_Great_Space_Operations\n",
      "Link: https://onepiece.fandom.com/wiki/CP9%27s_Independent_Report\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Sanji's_\"Resisting_in_Kamabakka\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Robin's_\"How_Terrible_You_People_Are\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Franky's_\"This_Week's_Me_is_No_Good\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Usopp's_\"I'll-Die-If-I'm-On-My-On_Disease\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Chopper's_\"I'm_Not_Food_You_Assholes\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Nami's_\"Weather_Report\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Brook's_\"Lodgings_and_Panties_Repayment\"\n",
      "Link: https://onepiece.fandom.com/wiki/Straw_Hat%27s_Separation_Serial#Zoro's_\"Where_the_Hell_Are_They?_What_a_Pain_in_the_Ass\"\n",
      "Link: https://onepiece.fandom.com/wiki/From_the_Decks_of_the_World\n",
      "Link: https://onepiece.fandom.com/wiki/Caribou%27s_Kehihihihi_in_the_New_World\n",
      "Link: https://onepiece.fandom.com/wiki/Solo_Journey_of_Jinbe,_Knight_of_the_Sea\n",
      "Link: https://onepiece.fandom.com/wiki/From_the_Decks_of_the_World:_The_500,000,000_Man_Arc\n",
      "Link: https://onepiece.fandom.com/wiki/The_Stories_of_the_Self-Proclaimed_Straw_Hat_Grand_Fleet\n",
      "Link: https://onepiece.fandom.com/wiki/%22Gang%22_Bege%27s_Oh_My_Family\n",
      "Link: https://onepiece.fandom.com/wiki/Germa_66%27s_Ahh..._An_Emotionless_Excursion\n"
     ]
    }
   ],
   "source": [
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "scraped_contents = []\n",
    "\n",
    "if table:\n",
    "    #Iterate through each row\n",
    "    for i, row in enumerate(table.find_all('tr')):\n",
    "        if i in [0,1, 8, 9]:\n",
    "            continue\n",
    "            \n",
    "        for cell in row.find_all(['th', 'td']):\n",
    "            cell_text = cell.get_text().strip()\n",
    "            print(cell_text)\n",
    "            a_tag = cell.find('a')\n",
    "            if a_tag and a_tag.has_attr('href'):\n",
    "                href = a_tag['href']\n",
    "                full_url = requests.compat.urljoin(response.url, href)  # Create full URL if necessary\n",
    "                print(f'Link: {full_url}')\n",
    "                scraped_content = scrape_page(full_url)\n",
    "                scraped_content = (scraped_content[0], f'{cell_text}\\n\\n\\n'+scraped_content[1])\n",
    "                scraped_contents.append(scraped_content)\n",
    "                # Now scrape the linked page for its title or other information\n",
    "        print('--- End of Row ---')\n",
    "\n",
    "#Next, find all the mini-series\n",
    "h4s = soup.find_all('h4')\n",
    "\n",
    "other_topics = ['Short-Term Focused Cover Page Serials']\n",
    "for h4 in h4s:\n",
    "    #Iterate through to find mini series\n",
    "    title = h4.get_text().strip().replace('[', '').replace(']', '')\n",
    "    if title in other_topics:\n",
    "        siblings = h4.find_next_siblings()\n",
    "\n",
    "        for sibling in siblings:\n",
    "            if sibling.name == 'ol':\n",
    "                ls = sibling.find_all('li')\n",
    "                for l in ls:\n",
    "                    a_tag = l.find('a')\n",
    "                    cell_text = l.get_text().strip()\n",
    "                    if a_tag and a_tag.has_attr('href'):\n",
    "                        href = a_tag['href']\n",
    "                        full_url = requests.compat.urljoin(response.url, href)  # Create full URL if necessary\n",
    "                        print(f'Link: {full_url}')\n",
    "                        scraped_content = scrape_page(full_url, topic = ['Long Summary', 'Story Impact'])\n",
    "                        scraped_content = (scraped_content[0], f'{cell_text}\\n\\n\\n'+scraped_content[1])\n",
    "                        scraped_contents.append(scraped_content)\n",
    "                break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separation, from the decks of the word, hatchan, gedatsu, miss goldenweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory ='database'\n",
    "\n",
    "for title, content in scraped_contents:\n",
    "\n",
    "    with open(f'{save_directory}/{title}.txt', 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store with the scraped database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 439.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader('database/', glob=\"*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49/49 [00:00<00:00, 171984.01it/s]\n"
     ]
    }
   ],
   "source": [
    "#Add Meta Data\n",
    "from tqdm import tqdm\n",
    "for doc in tqdm(docs):\n",
    "    doc.metadata[\"title\"] = re.split(\"/|.txt\", doc.metadata[\"source\"])[1]\n",
    "    doc.metadata[\"Chapters\"] = re.search(\"Chapters \\d+ to (?:\\d+|Current)\", doc.page_content)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store in Vector DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46709a1345945ed9905f57c7235c865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15903927989a47ee83fabe3414daf126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfae381727864baf9fd5cec51855b2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99be8ea006ed49b2a9cac8eafc88db1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f47ddf00aa64cf3a35bbc363a3ebbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b460ec9718349889519fce0936c14f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59d27f836194874b8b9b310a6c23d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b1ae91687b4afb8965eee3e4820257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21171efb6fc64fc9a883295de9eb99e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d223e24c4534fe9be466230945ee3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-small-v2\", \n",
    "    model_kwargs={\"device\":\"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\":True, \"batch_size\":128,}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1024, chunk_overlap = 0)\n",
    "\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "# db = FAISS.from_documents(split_documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': \"database/Ace's Great Blackbeard Search.txt\",\n",
       " 'title': \"Ace's Great Blackbeard Search\",\n",
       " 'Chapters': 'Chapters 272 to 305'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2283/2283 [00:04<00:00, 483.20it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('chunks', exist_ok = True)\n",
    "all_titles = set([doc.metadata['title'] for doc in split_documents])\n",
    "title_count = {t: 0 for t in all_titles}\n",
    "\n",
    "import json\n",
    "\n",
    "for doc in tqdm(split_documents):\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata\n",
    "\n",
    "    title = metadata['title']\n",
    "    filename = f'{title}-{title_count[title]}.json'\n",
    "\n",
    "    with open(f'chunks/{filename}', 'w') as f:\n",
    "        json.dump({'content': page_content, 'metadata': metadata}, f)\n",
    "\n",
    "    title_count[title] += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baratie Arc': 12,\n",
       " \"Enel's Great Space Operations\": 6,\n",
       " 'Fish-Man Island Arc': 256,\n",
       " 'Punk Hazard Arc': 155,\n",
       " 'Return to Sabaody Arc': 33,\n",
       " 'Marineford Arc': 117,\n",
       " 'Egghead Arc': 228,\n",
       " \"Ace's Great Blackbeard Search\": 4,\n",
       " 'Romance Dawn Arc': 25,\n",
       " 'Water 7 Arc': 48,\n",
       " \"Caribou's Kehihihihi in the New World\": 12,\n",
       " 'Levely Arc': 18,\n",
       " \"Jango's Dance Paradise\": 4,\n",
       " 'Impel Down Arc': 118,\n",
       " 'Post-War Arc': 88,\n",
       " 'Jaya Arc': 16,\n",
       " 'Syrup Village Arc': 13,\n",
       " 'Amazon Lily Arc': 49,\n",
       " 'Reverse Mountain Arc': 8,\n",
       " 'Whole Cake Island Arc': 92,\n",
       " 'From the Decks of the World': 7,\n",
       " 'Solo Journey of Jinbe, Knight of the Sea': 5,\n",
       " \"Germa 66's Ahh... An Emotionless Excursion\": 7,\n",
       " 'Thriller Bark Arc': 74,\n",
       " \"Wapol's Omnivorous Hurrah\": 3,\n",
       " 'Drum Island Arc': 11,\n",
       " \"Hatchan's Sea-Floor Stroll\": 1,\n",
       " 'Dressrosa Arc': 145,\n",
       " 'From the Decks of the World: The 500,000,000 Man Arc': 5,\n",
       " 'Skypiea Arc': 38,\n",
       " 'Little Garden Arc': 12,\n",
       " \"Gedatsu's Accidental Blue-Sea Life\": 1,\n",
       " 'Enies Lobby Arc': 58,\n",
       " 'Post-Enies Lobby Arc': 29,\n",
       " 'Sabaody Archipelago Arc': 86,\n",
       " \"Buggy's Crew Adventure Chronicles\": 3,\n",
       " 'Diary of Koby-Meppo': 2,\n",
       " 'Loguetown Arc': 10,\n",
       " 'The Stories of the Self-Proclaimed Straw Hat Grand Fleet': 4,\n",
       " '\"Gang\" Bege\\'s Oh My Family': 7,\n",
       " 'Long Ring Long Land Arc': 17,\n",
       " 'Miss Goldenweek\\'s \"Operation: Meet Baroque Works\"': 1,\n",
       " 'Wano Country Arc': 325,\n",
       " 'Whisky Peak Arc': 7,\n",
       " \"CP9's Independent Report\": 3,\n",
       " 'Orange Town Arc': 11,\n",
       " 'Arabasta Arc': 37,\n",
       " 'Arlong Park Arc': 20,\n",
       " 'Zou Arc': 52}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Just as the Emperor thought he had dealt with all his immediate enemies, Luffy rose again while revealing the epiphany he gained from taking Kaidou's last attack, that Haoshoku Haki can also be infused into attacks. Kaidou laughed and then attacked, while stating that only the mightiest can use such a technique. However, Luffy instantly countered, using Haoshoku-infused emissive Haki attacks, chaining a kick which knocked away the kanabo, a punch to Kaidou's gut followed by an uppercut, which floored the Emperor. Luffy then thanked Zoro and Law, telling them to go down, while he himself would take on Kaidou alone. The two clashed, both using Haoshoku Haki in their attacks. Unfortunately, due to Luffy's inexperience in using Haoshoku Haki in this way, he soon lost and was sent plummeting into the sea, with Kaidou commenting that he got carried away and should have cut of his head, since without it he couldn't break the morale of the rebels.\" metadata={'source': 'database/Wano Country Arc.txt', 'title': 'Wano Country Arc', 'Chapters': 'Chapters 909 to 1057'}\n"
     ]
    }
   ],
   "source": [
    "#Try searching with vector\n",
    "query = \"What was the name finishing move Luffy used against Kaido?\"\n",
    "embed_query = embeddings.embed_query(query)\n",
    "similar_docs = db.similarity_search_by_vector(embed_query, 20)\n",
    "\n",
    "print(similar_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5}, search_type='mmr')\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The finishing move Luffy used against Kaidou was Gomu Gomu no Kong Gun.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What was the name finishing move Luffy used against Kaido?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History-aware chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", '''Given the above conversation, generate a search query to look up in order to get information relevant to the conversation''')\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# chat_history = []\n",
    "response = retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Name every current Straw Hat Pirates as of your latest knowledge\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", '''Answer the user's questions based on the below context:\\n\\n{context}'''),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context provided, there is no information about Luffy defeating Kaido. The details shared do mention the potential danger Luffy poses to the World Government if he were to succeed in defeating Kaido, but there is no specific account of Luffy beating Kaido.\n"
     ]
    }
   ],
   "source": [
    "# chat_history = [HumanMessage(content=\"What is One Piece? and What is the latest arc of it?\"), AIMessage(content=\"One Piece is a manga and anime series created by Eiichiro Oda. The latest arc of One Piece is the Egghead Arc, which follows the aftermath of the Levely and the Raid on Onigashima, leading to major shifts across the world that could potentially lead to a global war.\")]\n",
    "response = retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"How did Luffy beat Kaido?\",\n",
    "    # \"input\": \"\"\n",
    "})\n",
    "\n",
    "print(response['answer'])\n",
    "chat_history.append(AIMessage(content = response['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of now in the One Piece manga and anime, Luffy has not yet defeated Kaido. The battle between Luffy and Kaido is still ongoing, and the outcome has not been revealed yet.')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('How did Luffy beat Kaido?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
